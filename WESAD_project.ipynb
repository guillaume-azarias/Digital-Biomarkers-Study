{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IdO5x8DMJrmm"
   },
   "source": [
    "# Anomaly detection using Facebook Prophet:\n",
    "\n",
    "**Medical background:**\n",
    "\n",
    "In the last decades, the miniaturization of wearable sensors and development of data transmission technologies have allowed to collect medically relevant data called digital biomarkers. This data is revolutionising our modern Medicine by offering new perspectives to better understand the human physiology and the possibility to better identify, even predict disease progression.\n",
    "Yet, the data collected by wearable sensors is, for technical reasons heterogeneous and cannot be directly translated into a meaningful clinical status. The relevance of proper data analysis is extremely critical as a wrong data analysis may lead to miss critical disease progression steps or lead to the wrong diagnostic. Therefore, the overall goal of this project is to integrate patients’ sensor data into one or several outcome measures which meaningfully recapitulate the clinical status of the patient.\n",
    "\n",
    "[Stress](https://en.wikipedia.org/wiki/Stress_(biology)) is a natural and physiological response to threat, challenge or physical and psychological barrier. In humans, the two major systems leading responding to stress are the autonomic nervous system and hypothalamic-pituitary-adrenal axis. The sympathetic nervous system, the stress-related part of the autonomic nervous system aims to distribute the energy to the most relevant body parts, to react to the stressor by fighting or escaping for instance. The hypothalamic-pituitary-adrenal axis regulates metabolic, psychological and immunological functions.\n",
    "The adrenaline alters the following: motion rate, electrocardiogram (ECG), electrodermal activity (EDA), electromyogram (EMG), respiration and body temperature.\n",
    "\n",
    "**Goal of the script:**\n",
    "\n",
    "Here, we aim leverage the power of artificial intelligence to reach a medical insight. Specifically, we want to detect the stress-induced biological changes from the wearable device measure with the highest sensiticity.\n",
    "\n",
    "**Motivations to use a forecasting method to detect activity:**\n",
    "\n",
    "Previous works demonstrated the ability to related self-labeled stress status to sensor data acquired by wearable sensors.\n",
    "Here we try a different approach assuming that physiological rythms are altered by stress. We are investigating if a time series forecasting method coupled with anomaly detection provides a more sensitive methods to detect stress-related changes.\n",
    "\n",
    "**Data format**\n",
    "\n",
    "The reader may read the original source of data here:\n",
    "- [UCI website](https://archive.ics.uci.edu/ml/datasets/WESAD+%28Wearable+Stress+and+Affect+Detection%29) (check the website shown below) to download the WESAD dataset \n",
    "- [wesad_readme file](wesad_readme.pdf) and [wesad poster](WESAD poster.pdf), both located together with the WESAD dataset\n",
    "\n",
    "**Structure of the code**\n",
    "\n",
    "    1 - Read the data\n",
    "    2 - Data preparation: segmentation per task, quality control\n",
    "    3 - Prediction of sensor data in the absence of change of stress status\n",
    "    4 - Detection of anomaly in the sensor's data, indicating a change in the stress status\n",
    "    4 - Cross-validation and boot-strapping assess the robustness of each candidate model and generate estimates of variability to facilitate model selection\n",
    "\n",
    "**Credit**\n",
    "\n",
    "- The data extraction part was modified from a script written by [aganjag](https://github.com/jaganjag) and is available [here](https://github.com/jaganjag/stress_affect_detection/blob/master/prototype.ipynb)\n",
    "- The implementation of Prophet for Time Series Forecasting was based on this [tutorial](https://medium.com/analytics-vidhya/time-series-forecast-anomaly-detection-with-facebook-prophet-558136be4b8d) written by Paul Lo. It makes use of the open-source project [Prophet](https://facebook.github.io/prophet/), a forecasting procedure implemented in R and Python, based on the paper of [Taylor and Letham, 2017](https://peerj.com/preprints/3190/).\n",
    "\n",
    "> Questions:\n",
    "> Contact Guillaume Azarias at guillaume.azarias@hotmail.com"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://archive.ics.uci.edu/ml/datasets/WESAD+%28Wearable+Stress+and+Affect+Detection%29', width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import qgrid\n",
    "\n",
    "# Note that the interactive plot may not work in Jupyter lab, but only in Jupyter Notebook (conflict of javascripts)\n",
    "%matplotlib widget "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n8FSgHwCa6bi"
   },
   "outputs": [],
   "source": [
    "import fbprophet\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.diagnostics import cross_validation, performance_metrics\n",
    "from fbprophet.plot import plot_cross_validation_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rKZsaWFTa9Km",
    "outputId": "15c0d68c-89da-4503-b0c4-98703d9a789a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbprophet.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "import itertools\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the functions from the helper.py\n",
    "from helper import load_ds, df_dev_formater, find_index, df_generator, prophet_fit, prophet_plot, get_outliers, prophet, GridSearch_Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the WESAD data\n",
    "\n",
    "The dimensions of the dataset depend on both the device and parameters:\n",
    "\n",
    "|     Device     | Location|Parameter|Acq. frequency|Number of dimensions|Data points (S5)| Duration (S5)|\n",
    "|:---------------|:-------:|:-------:|:------------:|:------------------:|:--------------:|:------------:|\n",
    "|**RespiBAN Pro**|chest    | ACC     |700Hz         |**3**               |4496100         |6'423sec      |\n",
    "|                |         | ECG     |\"             |1                   |                |              |\n",
    "|                |         | EDA     |\"             |1                   |                |              |\n",
    "|                |         | EMG     |\"             |1                   |                |              |\n",
    "|                |         | RESP    |\"             |1                   |                |              |\n",
    "|                |         | TEMP    |\"             |1                   |                |              |\n",
    "|                |         |         |              |                    |                |              |\n",
    "|**Empatica E4** |wrist    | ACC     |32Hz          |**3**               |200256          |6'258sec      |\n",
    "|                |         | BVP     |64Hz          |1                   |400512          |              |\n",
    "|                |         | EDA     |4Hz           |1                   |25032           |              |\n",
    "|                |         | TEMP    |4Hz           |1                   |25032           |              |\n",
    "\n",
    "*Note that ACC is a matrix of 3 dimensions for the 3 spatial dimensions.*\n",
    "\n",
    "*'ECG', 'EDA', 'EMG', 'Resp', 'Temp' have each 1 dimension.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "working_freq      4\n",
       "ACC_chest       700\n",
       "ECG_chest       700\n",
       "EDA_chest       700\n",
       "EMG_chest       700\n",
       "Resp_chest      700\n",
       "Temp_chest      700\n",
       "ACC_wrist        32\n",
       "BVP_wrist        64\n",
       "EDA_wrist         4\n",
       "TEMP_wrist        4\n",
       "label           700\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = np.array([4, 700, 700, 700, 700, 700, 700, 32, 64, 4, 4, 700])\n",
    "freq_df = pd.Series(freq, index= ['working_freq', 'ACC_chest', 'ECG_chest', 'EDA_chest', 'EMG_chest', 'Resp_chest', 'Temp_chest', 'ACC_wrist', 'BVP_wrist', 'EDA_wrist', 'TEMP_wrist', 'label'])\n",
    "freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'250L'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the working frequency, eg the frequency to adjust all data\n",
    "working_freq = str(int(1000/freq_df.loc['working_freq'])) + 'L'\n",
    "working_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* The class read_data_of_one_subject was originally written by [aganjag](https://github.com/jaganjag/stress_affect_detection/blob/master/prototype.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj_data[subject] = read_data_one_subject(data_set_path, subject)\n",
    "class read_data_of_one_subject:\n",
    "    \"\"\"Read data from WESAD dataset\"\"\"\n",
    "    def __init__(self, path, subject):\n",
    "        self.keys = ['label', 'subject', 'signal']\n",
    "        self.signal_keys = ['wrist', 'chest']\n",
    "        self.chest_sensor_keys = ['ACC', 'ECG', 'EDA', 'EMG', 'Resp', 'Temp']\n",
    "        self.wrist_sensor_keys = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
    "        os.chdir(path) # Change the current working directory to path\n",
    "        os.chdir(subject) # Change the current working directory to path. Why not using data_set_path ?\n",
    "        with open(subject + '.pkl', 'rb') as file: # with will automatically close the file after the nested block of code\n",
    "            data = pickle.load(file, encoding='latin1')\n",
    "        self.data = data\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.data[self.keys[0]]\n",
    "\n",
    "    def get_wrist_data(self):\n",
    "        \"\"\"\"\"\"\n",
    "        #label = self.data[self.keys[0]]\n",
    "        assert subject == self.data[self.keys[1]], 'WARNING: Mixing up the data from different persons'\n",
    "        signal = self.data[self.keys[2]]\n",
    "        wrist_data = signal[self.signal_keys[0]]\n",
    "        #wrist_ACC = wrist_data[self.wrist_sensor_keys[0]]\n",
    "        #wrist_ECG = wrist_data[self.wrist_sensor_keys[1]]\n",
    "        return wrist_data\n",
    "\n",
    "    def get_chest_data(self):\n",
    "        \"\"\"\"\"\"\n",
    "        assert subject == self.data[self.keys[1]], 'WARNING: Mixing up the data from different persons'\n",
    "        signal = self.data[self.keys[2]]\n",
    "        chest_data = signal[self.signal_keys[1]]\n",
    "        return chest_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_path = \"../../Data/WESAD\"\n",
    "subject = 'S5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_data = {}\n",
    "obj_data[subject] = read_data_of_one_subject(data_set_path, subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Workplan:*\n",
    "\n",
    "**A) Exploratory data analysis**\n",
    "\n",
    "    1) Discard for now the ACC data. Preliminary results on other parameters may guide the ways to investigate the accelerometer data\n",
    "    2) Get the study protocol\n",
    "    3) Use rolling.mean() to synchronise the data at the same frequency\n",
    "    4) Synchronise data\n",
    "    5) Include label data if possible\n",
    "    6) Plot data\n",
    "    7) Segmentation per task\n",
    "    8) Quality control \n",
    "\n",
    "**B) Perform time series forecasting**\n",
    "\n",
    "    1) ADCF test\n",
    "    2) Prophet\n",
    "    3) ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the study protocol\n",
    "*From the wesad_readme.pdf:*\n",
    "\n",
    "The order of the different conditions is defined on the second line in SX_quest.csv. Please refer to [1] for further details on each of the conditions (see Section 3.3 there). Please ignore the elements “bRead”, “fRead”, and “sRead”: these are not relevant for this dataset.\n",
    "The time interval of each condition is defined as start and end time, see the lines 3 and 4 in SX_quest.csv. Time is given in the format [minutes.seconds]. Time is counted from the start of the RespiBAN device’s start of recording."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study protocol from the _quest.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/guillaume/Documents/Projects/Data/WESAD/S5\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/guillaume/Documents/Projects/Data/WESAD/S5/S5_quest.csv\n"
     ]
    }
   ],
   "source": [
    "SX_quest_filename = os.getcwd() + '/' + subject + '_quest.csv'\n",
    "print(SX_quest_filename)\n",
    "# bp_data = pd.read_csv(\"/Users/guillaume/Documents/Projects/Data/WESAD/S2/S2_quest.csv\", header=1, delimiter=';')\n",
    "study_protocol_raw = pd.read_csv(SX_quest_filename, delimiter=';')\n",
    "# study_protocol_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.37</td>\n",
       "      <td>25.55</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.00</td>\n",
       "      <td>38.34</td>\n",
       "      <td>Fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.43</td>\n",
       "      <td>52.40</td>\n",
       "      <td>Medi 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61.00</td>\n",
       "      <td>72.05</td>\n",
       "      <td>TSST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92.15</td>\n",
       "      <td>99.12</td>\n",
       "      <td>Medi 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start    end    task\n",
       "0   5.37  25.55    Base\n",
       "1  32.00  38.34     Fun\n",
       "2  45.43  52.40  Medi 1\n",
       "3  61.00  72.05    TSST\n",
       "4  92.15  99.12  Medi 2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table with the interval of every steps\n",
    "study_protocol = study_protocol_raw.iloc[1:3, 1:6]\n",
    "study_protocol = study_protocol.transpose().astype(float)\n",
    "study_protocol.columns = ['start', 'end']\n",
    "study_protocol['task'] = study_protocol_raw.iloc[0, 1:6].transpose()\n",
    "study_protocol = study_protocol.reset_index(drop=True)\n",
    "study_protocol\n",
    "# study_protocol.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the time formatted as datetime\n",
    "# Note that the frequency chosen was 4Hz to match the lowest frequency of acquisition (250ms)\n",
    "total_duration = study_protocol.end.max()\n",
    "data = pd.DataFrame()\n",
    "begin_df = datetime.datetime(2020, 1, 1) # For reading convenience\n",
    "end_df = begin_df + timedelta(minutes=int(total_duration)) + timedelta(seconds=total_duration-int(total_duration))\n",
    "data['time'] = pd.date_range(start=begin_df, end=end_df, freq=working_freq).to_pydatetime().tolist()\n",
    "data['task'] = np.nan\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f0b52ad6b84bf38ff564443452ecf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QgridWidget(grid_options={'fullWidthRows': True, 'syncColumnCellResize': True, 'forceFitColumns': True, 'defau…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Annotate the task in the data['task']\n",
    "for row in range(study_protocol.shape[0]):\n",
    "    # Datetime index of the beginning of the task\n",
    "    begin_state = study_protocol.iloc[row, 0]\n",
    "    begin = begin_df + timedelta(minutes=int(begin_state)) + timedelta(seconds=begin_state-int(begin_state))\n",
    "\n",
    "    # Datetime index of the end of the task\n",
    "    end_state = study_protocol.iloc[row, 1]\n",
    "    end = begin_df + timedelta(minutes=int(end_state)) + timedelta(seconds=end_state-int(end_state))\n",
    "\n",
    "    # Fill the task column according to the begin and end of task\n",
    "    data.loc[(data['time'] >= begin) & (data['time'] <= end), 'task'] = study_protocol.iloc[row, 2]\n",
    "\n",
    "# Show data\n",
    "qgrid_widget = qgrid.show_grid(data, show_toolbar=True)\n",
    "qgrid.show_grid(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical representation of the study protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute an arbitrary value to a task for graphical display of the study protocol\n",
    "data_graph = data\n",
    "data_graph['arbitrary_index'] = np.zeros\n",
    "\n",
    "data_graph.loc[data_graph['task'] == 'Base', 'arbitrary_index'] = 1\n",
    "data_graph.loc[data_graph['task'] == 'Fun', 'arbitrary_index'] = 3\n",
    "data_graph.loc[data_graph['task'] == 'Medi 1', 'arbitrary_index'] = 4\n",
    "data_graph.loc[data_graph['task'] == 'TSST', 'arbitrary_index'] = 2\n",
    "data_graph.loc[data_graph['task'] == 'Medi 2', 'arbitrary_index'] = 4\n",
    "\n",
    "data_graph['arbitrary_index'] = pd.to_numeric(data_graph['arbitrary_index'], errors='coerce')\n",
    "\n",
    "# # Show data\n",
    "# qgrid_widget = qgrid.show_grid(data_graph, show_toolbar=True)\n",
    "# qgrid.show_grid(data_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251fcefbf86d4a93a279c2889c546930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "fig_sp, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "plt.plot('time', 'arbitrary_index', data=data_graph, color='darkblue', marker='o',linestyle='dashed', linewidth=0.5, markersize=2)\n",
    "plt.gcf().autofmt_xdate()\n",
    "myFmt = DateFormatter(\"%H:%M\")\n",
    "ax.xaxis.set_major_formatter(myFmt)\n",
    "plt.xlabel('Time elapsed (hh:mm)', fontsize=15)\n",
    "plt.ylim(0,6)\n",
    "plt.ylabel('Arbitrary index', fontsize=15)\n",
    "name = 'Study protocol for the subject ' + subject\n",
    "plt.title(name, fontsize=20)\n",
    "\n",
    "# Graph annotation\n",
    "for row in range(study_protocol.shape[0]):\n",
    "    # Datetime index of the beginning of the task\n",
    "    begin_state = study_protocol.iloc[row, 0]\n",
    "    begin = begin_df + timedelta(minutes=int(begin_state)) + timedelta(seconds=begin_state-int(begin_state))\n",
    "\n",
    "    # Datetime index of the end of the task\n",
    "    end_state = study_protocol.iloc[row, 1]\n",
    "    end = begin_df + timedelta(minutes=int(end_state)) + timedelta(seconds=end_state-int(end_state))\n",
    "\n",
    "    # Draw a rectangle and annotate the graph\n",
    "    ax.axvspan(begin, end, facecolor='b', alpha=0.2)\n",
    "    text_location = begin+((end-begin)/2)*1/2\n",
    "    ax.annotate(study_protocol.iloc[row, 2], xy=(begin, 5), xytext=(text_location, 5.5), fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the results of the self-assesment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Subj</th>\n",
       "      <th>S5</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>Unnamed: 20</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "      <th>Unnamed: 24</th>\n",
       "      <th>Unnamed: 25</th>\n",
       "      <th>Unnamed: 26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># ORDER</td>\n",
       "      <td>Base</td>\n",
       "      <td>Fun</td>\n",
       "      <td>Medi 1</td>\n",
       "      <td>TSST</td>\n",
       "      <td>Medi 2</td>\n",
       "      <td>bRead</td>\n",
       "      <td>fRead</td>\n",
       "      <td>sRead</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td># START</td>\n",
       "      <td>5.37</td>\n",
       "      <td>32</td>\n",
       "      <td>45.43</td>\n",
       "      <td>61</td>\n",
       "      <td>92.15</td>\n",
       "      <td>26.15</td>\n",
       "      <td>41.15</td>\n",
       "      <td>75.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td># END</td>\n",
       "      <td>25.55</td>\n",
       "      <td>38.34</td>\n",
       "      <td>52.4</td>\n",
       "      <td>72.05</td>\n",
       "      <td>99.12</td>\n",
       "      <td>27.47</td>\n",
       "      <td>42.45</td>\n",
       "      <td>76.32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td># PANAS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td># PANAS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td># PANAS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td># PANAS</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td># PANAS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td># STAI</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td># STAI</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td># STAI</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td># STAI</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td># STAI</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td># DIM</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td># DIM</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td># DIM</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td># DIM</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td># DIM</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td># SSSQ</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     # Subj     S5 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6  \\\n",
       "0   # ORDER   Base        Fun     Medi 1       TSST     Medi 2      bRead   \n",
       "1   # START   5.37         32      45.43         61      92.15      26.15   \n",
       "2     # END  25.55      38.34       52.4      72.05      99.12      27.47   \n",
       "3       NaN    NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "4   # PANAS      1          1          4          3          1          3   \n",
       "5   # PANAS      1          1          3          3          1          2   \n",
       "6   # PANAS      1          1          3          1          1          3   \n",
       "7   # PANAS      5          1          4          2          2          3   \n",
       "8   # PANAS      1          1          3          2          1          2   \n",
       "9       NaN    NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "10   # STAI      4          2          1          3          1          2   \n",
       "11   # STAI      3          1          2          2          1          2   \n",
       "12   # STAI      4          1          1          4          1          2   \n",
       "13   # STAI      1          3          2          1          2          2   \n",
       "14   # STAI      4          1          1          4          1          2   \n",
       "15      NaN    NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "16    # DIM      7          2        NaN        NaN        NaN        NaN   \n",
       "17    # DIM      6          2        NaN        NaN        NaN        NaN   \n",
       "18    # DIM      6          1        NaN        NaN        NaN        NaN   \n",
       "19    # DIM      5          8        NaN        NaN        NaN        NaN   \n",
       "20    # DIM      7          1        NaN        NaN        NaN        NaN   \n",
       "21      NaN    NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "22   # SSSQ      5          5          5          5          3          2   \n",
       "\n",
       "   Unnamed: 7 Unnamed: 8  Unnamed: 9  ...  Unnamed: 17  Unnamed: 18  \\\n",
       "0       fRead      sRead         NaN  ...          NaN          NaN   \n",
       "1       41.15      75.28         NaN  ...          NaN          NaN   \n",
       "2       42.45      76.32         NaN  ...          NaN          NaN   \n",
       "3         NaN        NaN         NaN  ...          NaN          NaN   \n",
       "4           1          1         1.0  ...          4.0          4.0   \n",
       "5           1          1         1.0  ...          1.0          3.0   \n",
       "6           1          1         1.0  ...          3.0          3.0   \n",
       "7           1          2         1.0  ...          3.0          4.0   \n",
       "8           1          1         1.0  ...          2.0          2.0   \n",
       "9         NaN        NaN         NaN  ...          NaN          NaN   \n",
       "10        NaN        NaN         NaN  ...          NaN          NaN   \n",
       "11        NaN        NaN         NaN  ...          NaN          NaN   \n",
       "12        NaN        NaN         NaN  ...          NaN          NaN   \n",
       "13        NaN        NaN         NaN  ...          NaN          NaN   \n",
       "14        NaN        NaN         NaN  ...          NaN          NaN   \n",
       "15        NaN        NaN         NaN  ...          NaN          NaN   \n",
       "16        NaN        NaN         NaN  ...          NaN          NaN   \n",
       "17        NaN        NaN         NaN  ...          NaN          NaN   \n",
       "18        NaN        NaN         NaN  ...          NaN          NaN   \n",
       "19        NaN        NaN         NaN  ...          NaN          NaN   \n",
       "20        NaN        NaN         NaN  ...          NaN          NaN   \n",
       "21        NaN        NaN         NaN  ...          NaN          NaN   \n",
       "22        NaN        NaN         NaN  ...          NaN          NaN   \n",
       "\n",
       "    Unnamed: 19  Unnamed: 20  Unnamed: 21  Unnamed: 22 Unnamed: 23  \\\n",
       "0           NaN          NaN          NaN          NaN         NaN   \n",
       "1           NaN          NaN          NaN          NaN         NaN   \n",
       "2           NaN          NaN          NaN          NaN         NaN   \n",
       "3           NaN          NaN          NaN          NaN         NaN   \n",
       "4           1.0          1.0          2.0          1.0         3.0   \n",
       "5           1.0          1.0          2.0          1.0         2.0   \n",
       "6           1.0          1.0          1.0          1.0         4.0   \n",
       "7           3.0          1.0          4.0          1.0         2.0   \n",
       "8           1.0          1.0          1.0          1.0         4.0   \n",
       "9           NaN          NaN          NaN          NaN         NaN   \n",
       "10          NaN          NaN          NaN          NaN         NaN   \n",
       "11          NaN          NaN          NaN          NaN         NaN   \n",
       "12          NaN          NaN          NaN          NaN         NaN   \n",
       "13          NaN          NaN          NaN          NaN         NaN   \n",
       "14          NaN          NaN          NaN          NaN         NaN   \n",
       "15          NaN          NaN          NaN          NaN         NaN   \n",
       "16          NaN          NaN          NaN          NaN         NaN   \n",
       "17          NaN          NaN          NaN          NaN         NaN   \n",
       "18          NaN          NaN          NaN          NaN         NaN   \n",
       "19          NaN          NaN          NaN          NaN         NaN   \n",
       "20          NaN          NaN          NaN          NaN         NaN   \n",
       "21          NaN          NaN          NaN          NaN         NaN   \n",
       "22          NaN          NaN          NaN          NaN         NaN   \n",
       "\n",
       "    Unnamed: 24  Unnamed: 25  Unnamed: 26  \n",
       "0           NaN          NaN          NaN  \n",
       "1           NaN          NaN          NaN  \n",
       "2           NaN          NaN          NaN  \n",
       "3           NaN          NaN          NaN  \n",
       "4           1.0          NaN          NaN  \n",
       "5           1.0          NaN          NaN  \n",
       "6           1.0          NaN          NaN  \n",
       "7           1.0          2.0          1.0  \n",
       "8           1.0          NaN          NaN  \n",
       "9           NaN          NaN          NaN  \n",
       "10          NaN          NaN          NaN  \n",
       "11          NaN          NaN          NaN  \n",
       "12          NaN          NaN          NaN  \n",
       "13          NaN          NaN          NaN  \n",
       "14          NaN          NaN          NaN  \n",
       "15          NaN          NaN          NaN  \n",
       "16          NaN          NaN          NaN  \n",
       "17          NaN          NaN          NaN  \n",
       "18          NaN          NaN          NaN  \n",
       "19          NaN          NaN          NaN  \n",
       "20          NaN          NaN          NaN  \n",
       "21          NaN          NaN          NaN  \n",
       "22          NaN          NaN          NaN  \n",
       "\n",
       "[23 rows x 27 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_protocol_raw\n",
    "# Show data\n",
    "# qgrid_widget = qgrid.show_grid(study_protocol_raw, show_toolbar=True)\n",
    "# qgrid.show_grid(study_protocol_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The order of result of the self-assessment is listed below:**\n",
    "- line 0: Condition\n",
    "- lines 1-2: Start and end of the condition\n",
    "- line 3: *NaN line for data separation*\n",
    "- lines 4-8: PANAS result with the 26 different feeling in columns (columns 1-27), and scores (1 = Not at all, 2 = A little bit, 3 = Somewhat, 4 = Very much, 5 = Extremely) for the conditions: Base (line 4) Fun (line 5) Medi 1 (line 6) TSST (line 7) and Medi 2 (line 8). *Note that there are 2 more features for the Stress condition only.*\n",
    "- line 9: *NaN line for data separation*\n",
    "- lines 10-14: STAI result with the 6 different feelings in columns and scores (1 = Not at all, 2 = Somewhat, 3 = Moderately so, 4 = Very much so) for the conditions: Base (line 10) Fun (line 11) Medi 1 (line 12) TSST (line 13) and Medi 2 (line 14). \n",
    "- line 15: *NaN line for data separation*\n",
    "- lines 16-20: SAM (Self-Assessment Manikins) results with the 2 different feelings (valence and arousal) in columns  for the conditions: Base (line 16) Fun (line 17) Medi 1 (line 18) TSST (line 19) and Medi 2 (line 20).\n",
    "- line 21: *NaN line for data separation*\n",
    "- lines 22: SSSQ result with the 6 different feeling and scores (1 = Not at all, 2 = A little bit, 3 = Somewhat, 4 = Very much, 5 = Extremely) for the stress condition only.\n",
    "\n",
    "*TO DO*: pool, transpose and normalise data. Verify SSSQ information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexingError",
     "evalue": "Too many indexers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-42d790582ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Base results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mself_assessment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy_protocol_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mself_assessment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_setitem_indexer\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, raise_missing)\u001b[0m\n\u001b[1;32m   2152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2153\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2154\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2155\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1995\u001b[0m             \u001b[0;31m# a tuple should already have been caught by this point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1996\u001b[0m             \u001b[0;31m# so don't treat a tuple as a valid indexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1997\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Too many indexers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1998\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1999\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexingError\u001b[0m: Too many indexers"
     ]
    }
   ],
   "source": [
    "self_assessment = pd.DataFrame()\n",
    "self_assessment = study_protocol_raw.iloc[0, 1:5].T\n",
    "\n",
    "# Base results\n",
    "self_assessment.iloc[0, 1:27] = study_protocol_raw.iloc[4, 1:27]\n",
    "self_assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the wrist data and adjust to the working frequency (4Hz)\n",
    "\n",
    "|     Device     | Location|Parameter|Acq. frequency|Number of dimensions|Data points (S5)| Duration (S5)|\n",
    "|:---------------|:-------:|:-------:|:------------:|:------------------:|:--------------:|:------------:|\n",
    "|**Empatica E4** |wrist    | ACC     |32Hz          |**3**               |200256          |6'258sec      |\n",
    "|                |         | BVP     |64Hz          |1                   |400512          |              |\n",
    "|                |         | EDA     |4Hz           |1                   |25032           |              |\n",
    "|                |         | TEMP    |4Hz           |1                   |25032           |              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "working_freq      4\n",
       "ACC_chest       700\n",
       "ECG_chest       700\n",
       "EDA_chest       700\n",
       "EMG_chest       700\n",
       "Resp_chest      700\n",
       "Temp_chest      700\n",
       "ACC_wrist        32\n",
       "BVP_wrist        64\n",
       "EDA_wrist         4\n",
       "TEMP_wrist        4\n",
       "label           700\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wrist_data_dict = obj_data[subject].get_wrist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original numbers of data per parameter: {'ACC': 200256, 'BVP': 400512, 'EDA': 25032, 'TEMP': 25032}\n",
      "Resampled data adjusted to 4Hz in the pandas DataFrame df_wrist:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC_wrist_x</th>\n",
       "      <th>ACC_wrist_y</th>\n",
       "      <th>ACC_wrist_z</th>\n",
       "      <th>BVP_wrist</th>\n",
       "      <th>EDA_wrist</th>\n",
       "      <th>TEMP_wrist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00.000</th>\n",
       "      <td>-16.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>-7.25</td>\n",
       "      <td>0.547723</td>\n",
       "      <td>34.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00.250</th>\n",
       "      <td>60.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-8.24</td>\n",
       "      <td>0.481218</td>\n",
       "      <td>34.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00.500</th>\n",
       "      <td>45.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>-11.14</td>\n",
       "      <td>0.518239</td>\n",
       "      <td>34.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00.750</th>\n",
       "      <td>72.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-32.96</td>\n",
       "      <td>0.440223</td>\n",
       "      <td>34.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:01.000</th>\n",
       "      <td>51.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>79.55</td>\n",
       "      <td>0.391623</td>\n",
       "      <td>34.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:44:16.750</th>\n",
       "      <td>45.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-26.64</td>\n",
       "      <td>1.004306</td>\n",
       "      <td>31.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:44:17.000</th>\n",
       "      <td>45.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>0.997912</td>\n",
       "      <td>31.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:44:17.250</th>\n",
       "      <td>46.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-22.00</td>\n",
       "      <td>0.985122</td>\n",
       "      <td>31.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:44:17.500</th>\n",
       "      <td>45.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-2.90</td>\n",
       "      <td>0.965938</td>\n",
       "      <td>31.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:44:17.750</th>\n",
       "      <td>45.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-22.90</td>\n",
       "      <td>0.955706</td>\n",
       "      <td>31.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25032 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ACC_wrist_x  ACC_wrist_y  ACC_wrist_z  BVP_wrist  \\\n",
       "2020-01-01 00:00:00.000        -16.0        -45.0        127.0      -7.25   \n",
       "2020-01-01 00:00:00.250         60.0         45.0         -1.0      -8.24   \n",
       "2020-01-01 00:00:00.500         45.0         41.0        -32.0     -11.14   \n",
       "2020-01-01 00:00:00.750         72.0         33.0          7.0     -32.96   \n",
       "2020-01-01 00:00:01.000         51.0         39.0          9.0      79.55   \n",
       "...                              ...          ...          ...        ...   \n",
       "2020-01-01 01:44:16.750         45.0         39.0         21.0     -26.64   \n",
       "2020-01-01 01:44:17.000         45.0         39.0         21.0      -0.84   \n",
       "2020-01-01 01:44:17.250         46.0         38.0         21.0     -22.00   \n",
       "2020-01-01 01:44:17.500         45.0         39.0         20.0      -2.90   \n",
       "2020-01-01 01:44:17.750         45.0         38.0         20.0     -22.90   \n",
       "\n",
       "                         EDA_wrist  TEMP_wrist  \n",
       "2020-01-01 00:00:00.000   0.547723       34.09  \n",
       "2020-01-01 00:00:00.250   0.481218       34.09  \n",
       "2020-01-01 00:00:00.500   0.518239       34.09  \n",
       "2020-01-01 00:00:00.750   0.440223       34.11  \n",
       "2020-01-01 00:00:01.000   0.391623       34.11  \n",
       "...                            ...         ...  \n",
       "2020-01-01 01:44:16.750   1.004306       31.03  \n",
       "2020-01-01 01:44:17.000   0.997912       31.03  \n",
       "2020-01-01 01:44:17.250   0.985122       31.03  \n",
       "2020-01-01 01:44:17.500   0.965938       31.03  \n",
       "2020-01-01 01:44:17.750   0.955706       31.01  \n",
       "\n",
       "[25032 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extraction of numbers of data\n",
    "wrist_dict_length = {key: len(value) for key, value in wrist_data_dict.items()}\n",
    "print('Original numbers of data per parameter: ' + str(wrist_dict_length))\n",
    "wrist_ser_length = pd.Series(wrist_dict_length)\n",
    "df_wrist = pd.DataFrame()\n",
    "\n",
    "# Adjust all data to the same frequence\n",
    "for wrist_param, param_length in wrist_ser_length.items():\n",
    "    # Generate the frequence in microseconds (U) from the acquisition frequency\n",
    "    freq = str(int(1000000/freq_df.loc[wrist_param + '_wrist'])) + 'U'\n",
    "    # Generate temporary dataset \n",
    "    index = pd.date_range(start='1/1/2020', periods=param_length, freq=freq)\n",
    "    df_temp_raw = pd.DataFrame(wrist_data_dict[wrist_param], index=index)\n",
    "    if wrist_param == 'ACC':\n",
    "        df_temp_raw.columns = ['ACC_wrist_x', 'ACC_wrist_y', 'ACC_wrist_z']\n",
    "    else:\n",
    "        df_temp_raw.columns = [wrist_param + '_wrist']\n",
    "    # Resampling\n",
    "    df_temp = df_temp_raw.resample(working_freq).pad()\n",
    "    # Append the wrist data\n",
    "    if df_wrist.shape[1]==0:\n",
    "        df_wrist = df_temp\n",
    "    else:\n",
    "        df_wrist = pd.concat([df_wrist, df_temp], axis=1)\n",
    "\n",
    "print('Resampled data adjusted to ' + str(freq_df['working_freq']) + 'Hz in the pandas DataFrame df_wrist:')\n",
    "df_wrist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chest data from the .pkl file adjusted to 4Hz\n",
    "\n",
    "|     Device     | Location|Parameter|Acq. frequency|Number of dimensions|Data points (S5)| Duration (S5)|\n",
    "|:---------------|:-------:|:-------:|:------------:|:------------------:|:--------------:|:------------:|\n",
    "|**RespiBAN Pro**|chest    | ACC     |700Hz         |**3**               |4496100         |6'423sec      |\n",
    "|                |         | ECG     |\"             |1                   |                |              |\n",
    "|                |         | EDA     |\"             |1                   |                |              |\n",
    "|                |         | EMG     |\"             |1                   |                |              |\n",
    "|                |         | RESP    |\"             |1                   |                |              |\n",
    "|                |         | TEMP    |\"             |1                   |                |              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chest_data_dict = obj_data[subject].get_chest_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original numbers of data per parameter: {'ACC': 4380600, 'ECG': 4380600, 'EMG': 4380600, 'EDA': 4380600, 'Temp': 4380600, 'Resp': 4380600}\n",
      "Resampled data adjusted to 4Hz in the pandas DataFrame df_chest:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC_chest_x</th>\n",
       "      <th>ACC_chest_y</th>\n",
       "      <th>ACC_chest_z</th>\n",
       "      <th>ECG_chest</th>\n",
       "      <th>EMG_chest</th>\n",
       "      <th>EDA_chest</th>\n",
       "      <th>Temp_chest</th>\n",
       "      <th>Resp_chest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00.000</th>\n",
       "      <td>0.8606</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>0.8570</td>\n",
       "      <td>-0.275803</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>3.888321</td>\n",
       "      <td>34.119934</td>\n",
       "      <td>0.044250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00.250</th>\n",
       "      <td>0.9122</td>\n",
       "      <td>-0.0464</td>\n",
       "      <td>-0.1102</td>\n",
       "      <td>0.125427</td>\n",
       "      <td>-0.001144</td>\n",
       "      <td>3.870010</td>\n",
       "      <td>34.130615</td>\n",
       "      <td>-0.456238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00.500</th>\n",
       "      <td>0.7784</td>\n",
       "      <td>0.1914</td>\n",
       "      <td>-0.1982</td>\n",
       "      <td>-0.018127</td>\n",
       "      <td>-0.004807</td>\n",
       "      <td>3.883743</td>\n",
       "      <td>34.130615</td>\n",
       "      <td>-0.331116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00.750</th>\n",
       "      <td>0.9096</td>\n",
       "      <td>-0.0700</td>\n",
       "      <td>-0.1038</td>\n",
       "      <td>0.064774</td>\n",
       "      <td>-0.004211</td>\n",
       "      <td>3.874588</td>\n",
       "      <td>34.129089</td>\n",
       "      <td>0.450134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:01.000</th>\n",
       "      <td>0.9066</td>\n",
       "      <td>-0.0538</td>\n",
       "      <td>-0.0962</td>\n",
       "      <td>-0.045090</td>\n",
       "      <td>-0.012222</td>\n",
       "      <td>3.880310</td>\n",
       "      <td>34.086426</td>\n",
       "      <td>1.849365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:44:14.250</th>\n",
       "      <td>0.9118</td>\n",
       "      <td>-0.0464</td>\n",
       "      <td>-0.1032</td>\n",
       "      <td>-0.006271</td>\n",
       "      <td>-0.010391</td>\n",
       "      <td>10.274506</td>\n",
       "      <td>34.977234</td>\n",
       "      <td>-3.100586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:44:14.500</th>\n",
       "      <td>0.9102</td>\n",
       "      <td>-0.0530</td>\n",
       "      <td>-0.1064</td>\n",
       "      <td>-0.072647</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>10.261917</td>\n",
       "      <td>34.877014</td>\n",
       "      <td>-2.728271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:44:14.750</th>\n",
       "      <td>0.9016</td>\n",
       "      <td>-0.0486</td>\n",
       "      <td>-0.1166</td>\n",
       "      <td>0.013367</td>\n",
       "      <td>-0.006042</td>\n",
       "      <td>10.252380</td>\n",
       "      <td>34.949463</td>\n",
       "      <td>-0.717163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:44:15.000</th>\n",
       "      <td>0.9054</td>\n",
       "      <td>-0.0590</td>\n",
       "      <td>-0.1270</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>-0.014648</td>\n",
       "      <td>10.234451</td>\n",
       "      <td>34.978790</td>\n",
       "      <td>1.863098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:44:15.250</th>\n",
       "      <td>0.8954</td>\n",
       "      <td>-0.0530</td>\n",
       "      <td>-0.1282</td>\n",
       "      <td>-0.101715</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>10.234451</td>\n",
       "      <td>34.964874</td>\n",
       "      <td>4.629517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25022 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ACC_chest_x  ACC_chest_y  ACC_chest_z  ECG_chest  \\\n",
       "2020-01-01 00:00:00.000       0.8606       0.0742       0.8570  -0.275803   \n",
       "2020-01-01 00:00:00.250       0.9122      -0.0464      -0.1102   0.125427   \n",
       "2020-01-01 00:00:00.500       0.7784       0.1914      -0.1982  -0.018127   \n",
       "2020-01-01 00:00:00.750       0.9096      -0.0700      -0.1038   0.064774   \n",
       "2020-01-01 00:00:01.000       0.9066      -0.0538      -0.0962  -0.045090   \n",
       "...                              ...          ...          ...        ...   \n",
       "2020-01-01 01:44:14.250       0.9118      -0.0464      -0.1032  -0.006271   \n",
       "2020-01-01 01:44:14.500       0.9102      -0.0530      -0.1064  -0.072647   \n",
       "2020-01-01 01:44:14.750       0.9016      -0.0486      -0.1166   0.013367   \n",
       "2020-01-01 01:44:15.000       0.9054      -0.0590      -0.1270   0.000504   \n",
       "2020-01-01 01:44:15.250       0.8954      -0.0530      -0.1282  -0.101715   \n",
       "\n",
       "                         EMG_chest  EDA_chest  Temp_chest  Resp_chest  \n",
       "2020-01-01 00:00:00.000   0.016800   3.888321   34.119934    0.044250  \n",
       "2020-01-01 00:00:00.250  -0.001144   3.870010   34.130615   -0.456238  \n",
       "2020-01-01 00:00:00.500  -0.004807   3.883743   34.130615   -0.331116  \n",
       "2020-01-01 00:00:00.750  -0.004211   3.874588   34.129089    0.450134  \n",
       "2020-01-01 00:00:01.000  -0.012222   3.880310   34.086426    1.849365  \n",
       "...                            ...        ...         ...         ...  \n",
       "2020-01-01 01:44:14.250  -0.010391  10.274506   34.977234   -3.100586  \n",
       "2020-01-01 01:44:14.500   0.001419  10.261917   34.877014   -2.728271  \n",
       "2020-01-01 01:44:14.750  -0.006042  10.252380   34.949463   -0.717163  \n",
       "2020-01-01 01:44:15.000  -0.014648  10.234451   34.978790    1.863098  \n",
       "2020-01-01 01:44:15.250  -0.000137  10.234451   34.964874    4.629517  \n",
       "\n",
       "[25022 rows x 8 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extraction of numbers of data\n",
    "chest_dict_length = {key: len(value) for key, value in chest_data_dict.items()}\n",
    "print('Original numbers of data per parameter: ' + str(chest_dict_length))\n",
    "chest_ser_length = pd.Series(chest_dict_length)\n",
    "df_chest = pd.DataFrame()\n",
    "\n",
    "# Adjust all data to the same frequence\n",
    "for chest_param, param_length in chest_ser_length.items():\n",
    "    # Generate the frequence in microseconds (U) from the acquisition frequency\n",
    "    freq = str(int(1000000/freq_df.loc[chest_param + '_chest'])) + 'U'\n",
    "    # Generate temporary dataset \n",
    "    index = pd.date_range(start='1/1/2020', periods=param_length, freq=freq)\n",
    "    df_temp_raw = pd.DataFrame(chest_data_dict[chest_param], index=index)\n",
    "    if chest_param == 'ACC':\n",
    "        df_temp_raw.columns = ['ACC_chest_x', 'ACC_chest_y', 'ACC_chest_z']\n",
    "    else:\n",
    "        df_temp_raw.columns = [chest_param + '_chest']\n",
    "    # Resampling\n",
    "    df_temp = df_temp_raw.resample(working_freq).pad()\n",
    "    # Append the chest data\n",
    "    if df_chest.shape[1]==0:\n",
    "        df_chest = df_temp\n",
    "    else:\n",
    "        df_chest = pd.concat([df_chest, df_temp], axis=1)\n",
    "\n",
    "print('Resampled data adjusted to ' + str(freq_df['working_freq']) + 'Hz in the pandas DataFrame df_chest:')\n",
    "df_chest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the label data\n",
    "\n",
    "‘label’: ID of the respective study protocol condition, sampled at 700 Hz.\n",
    "The following IDs are provided:\n",
    "    - 0 = not defined / transient\n",
    "    - 1 = baseline\n",
    "    - 2 = stress\n",
    "    - 3 = amusement\n",
    "    - 4 = meditation\n",
    "    - 5/6/7 = should be ignored in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "labels[subject] = obj_data[subject].get_labels()\n",
    "labels_dict = obj_data[subject].get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00.000</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01 00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00.250</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01 00:00:00.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00.500</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01 00:00:00.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00.750</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01 00:00:00.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:01.000</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01 00:00:01.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:44:14.250</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01 01:44:14.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:44:14.500</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01 01:44:14.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:44:14.750</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01 01:44:14.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:44:15.000</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01 01:44:15.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:44:15.250</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01 01:44:15.250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25022 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         label                    time\n",
       "2020-01-01 00:00:00.000      0 2020-01-01 00:00:00.000\n",
       "2020-01-01 00:00:00.250      0 2020-01-01 00:00:00.250\n",
       "2020-01-01 00:00:00.500      0 2020-01-01 00:00:00.500\n",
       "2020-01-01 00:00:00.750      0 2020-01-01 00:00:00.750\n",
       "2020-01-01 00:00:01.000      0 2020-01-01 00:00:01.000\n",
       "...                        ...                     ...\n",
       "2020-01-01 01:44:14.250      0 2020-01-01 01:44:14.250\n",
       "2020-01-01 01:44:14.500      0 2020-01-01 01:44:14.500\n",
       "2020-01-01 01:44:14.750      0 2020-01-01 01:44:14.750\n",
       "2020-01-01 01:44:15.000      0 2020-01-01 01:44:15.000\n",
       "2020-01-01 01:44:15.250      0 2020-01-01 01:44:15.250\n",
       "\n",
       "[25022 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = str(int(1000000/freq_df.loc['label'])) + 'U' # U means microseconds\n",
    "index = pd.date_range(start='1/1/2020', periods=len(labels_dict), freq=freq)\n",
    "df_label = pd.DataFrame(labels_dict, index=index)\n",
    "df_label = df_label.resample(working_freq).pad()\n",
    "df_label['time'] = df_label.index\n",
    "df_label.columns = ['label', 'time']\n",
    "# Ignore 5/6/7\n",
    "df_label.loc[df_label['label'] > 4, 'label'] = 0\n",
    "df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8680f779f834a76963a63e92fda76d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "fig_sp, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "plt.plot('time', 'label', data=df_label, color='darkblue', marker='o',linestyle='dashed', linewidth=0.5, markersize=2)\n",
    "plt.gcf().autofmt_xdate()\n",
    "myFmt = DateFormatter(\"%H:%M\")\n",
    "ax.xaxis.set_major_formatter(myFmt)\n",
    "plt.xlabel('Time elapsed (hh:mm)', fontsize=15)\n",
    "plt.ylim(0,6)\n",
    "plt.ylabel('Label', fontsize=15)\n",
    "name = 'Label data for the subject ' + subject\n",
    "plt.title(name, fontsize=20)\n",
    "\n",
    "# Graph annotation\n",
    "for row in range(study_protocol.shape[0]):\n",
    "    # Datetime index of the beginning of the task\n",
    "    begin_state = study_protocol.iloc[row, 0]\n",
    "    begin = begin_df + timedelta(minutes=int(begin_state)) + timedelta(seconds=begin_state-int(begin_state))\n",
    "\n",
    "    # Datetime index of the end of the task\n",
    "    end_state = study_protocol.iloc[row, 1]\n",
    "    end = begin_df + timedelta(minutes=int(end_state)) + timedelta(seconds=end_state-int(end_state))\n",
    "\n",
    "    # Draw a rectangle and annotate the graph\n",
    "    ax.axvspan(begin, end, facecolor='b', alpha=0.2)\n",
    "    text_location = begin+((end-begin)/2)*1/2\n",
    "    ax.annotate(study_protocol.iloc[row, 2], xy=(begin, 5), xytext=(text_location, 5.5), fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There is a lag between the data extracted from the _quest.csv file and the synchronized data:\n",
    "    - Check if there is any synchronisation data in the pkl file\n",
    "    - Look how to properly merge the datasets that don't have the same dimensions !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_one(chest_data_dict, idx, l_condition=0):\n",
    "    ecg_data = chest_data_dict[\"ECG\"][idx].flatten()\n",
    "    ecg_features = extract_mean_std_features(ecg_data, label=l_condition)\n",
    "    #print(ecg_features.shape)\n",
    "\n",
    "    eda_data = chest_data_dict[\"EDA\"][idx].flatten()\n",
    "    eda_features = extract_mean_std_features(eda_data, label=l_condition)\n",
    "    #print(eda_features.shape)\n",
    "\n",
    "    emg_data = chest_data_dict[\"EMG\"][idx].flatten()\n",
    "    emg_features = extract_mean_std_features(emg_data, label=l_condition)\n",
    "    #print(emg_features.shape)\n",
    "\n",
    "    temp_data = chest_data_dict[\"Temp\"][idx].flatten()\n",
    "    temp_features = extract_mean_std_features(temp_data, label=l_condition)\n",
    "    #print(temp_features.shape)\n",
    "\n",
    "    baseline_data = np.hstack((eda_features, temp_features, ecg_features, emg_features))\n",
    "    #print(len(baseline_data))\n",
    "    label_array = np.full(len(baseline_data), l_condition)\n",
    "    #print(label_array.shape)\n",
    "    #print(baseline_data.shape)\n",
    "    baseline_data = np.column_stack((baseline_data, label_array))\n",
    "    #print(baseline_data.shape)\n",
    "    return baseline_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute():\n",
    "#     data_set_path = \"/media/jac/New Volume/Datasets/WESAD\"\n",
    "    data_set_path = \"../../../Data/WESAD\"\n",
    "    file_path = \"ecg.txt\"\n",
    "    subject = 'S3' # Why defining subject here since it is defined 6 lines later in a loop ?\n",
    "    obj_data = {}\n",
    "    labels = {}\n",
    "    all_data = {}\n",
    "    subs = [2, 3, 4, 5, 6]\n",
    "    for i in subs:\n",
    "        subject = 'S' + str(i)\n",
    "        print(\"Reading data\", subject)\n",
    "        obj_data[subject] = read_data_one_subject(data_set_path, subject)\n",
    "        labels[subject] = obj_data[subject].get_labels()\n",
    "\n",
    "        wrist_data_dict = obj_data[subject].get_wrist_data()\n",
    "        wrist_dict_length = {key: len(value) for key, value in wrist_data_dict.items()}\n",
    "\n",
    "        chest_data_dict = obj_data[subject].get_chest_data()\n",
    "        chest_dict_length = {key: len(value) for key, value in chest_data_dict.items()}\n",
    "        print(chest_dict_length)\n",
    "        chest_data = np.concatenate((chest_data_dict['ACC'], chest_data_dict['ECG'], chest_data_dict['EDA'],\n",
    "                                     chest_data_dict['EMG'], chest_data_dict['Resp'], chest_data_dict['Temp']), axis=1)\n",
    "        # Get labels\n",
    "\n",
    "\n",
    "        # 'ACC' : 3, 'ECG' 1: , 'EDA' : 1, 'EMG': 1, 'RESP': 1, 'Temp': 1  ===> Total dimensions : 8\n",
    "        # No. of Labels ==> 8 ; 0 = not defined / transient, 1 = baseline, 2 = stress, 3 = amusement,\n",
    "        # 4 = meditation, 5/6/7 = should be ignored in this dataset\n",
    "\n",
    "        # Do for each subject\n",
    "        baseline = np.asarray([idx for idx, val in enumerate(labels[subject]) if val == 1])\n",
    "        # print(\"Baseline:\", chest_data_dict['ECG'][baseline].shape)\n",
    "        # print(baseline.shape)\n",
    "\n",
    "        stress = np.asarray([idx for idx, val in enumerate(labels[subject]) if val == 2])\n",
    "        # print(stress.shape)\n",
    "\n",
    "        amusement = np.asarray([idx for idx, val in enumerate(labels[subject]) if val == 3])\n",
    "        # print(amusement.shape)\n",
    "\n",
    "        baseline_data = extract_one(chest_data_dict, baseline, l_condition=1)\n",
    "        stress_data = extract_one(chest_data_dict, stress, l_condition=2)\n",
    "        amusement_data = extract_one(chest_data_dict, amusement, l_condition=3)\n",
    "\n",
    "        full_data = np.vstack((baseline_data, stress_data, amusement_data))\n",
    "        print(\"One subject data\", full_data.shape)\n",
    "        all_data[subject] = full_data\n",
    "\n",
    "    i = 0\n",
    "    for k, v in all_data.items():\n",
    "        if i == 0:\n",
    "            data = all_data[k]\n",
    "            i += 1\n",
    "        print(all_data[k].shape)\n",
    "        data = np.vstack((data, all_data[k]))\n",
    "\n",
    "    print(data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "ecg, eda = chest_data_dict['ECG'], chest_data_dict['EDA']\n",
    "x = [i for i in range(len(baseline))]\n",
    "for one in baseline:\n",
    "    x = [i for i in range(99)]\n",
    "    plt.plot(x, ecg[one:100])\n",
    "    break\n",
    "# \"\"\"\n",
    "\n",
    "x = [i for i in range(10000)]\n",
    "plt.plot(x, chest_data_dict['ECG'][:10000])\n",
    "plt.show()\n",
    "\n",
    "# BASELINE\n",
    "\n",
    "                                   [ecg_features[k] for k in ecg_features.keys()])\n",
    "\n",
    "ecg = nk.ecg_process(ecg=ecg_data, rsp=chest_data_dict['Resp'][baseline].flatten(), sampling_rate=700)\n",
    "print(os.getcwd())\n",
    "\n",
    "# \"\"\"\n",
    "recur_print\n",
    "print(type(ecg))\n",
    "print(ecg.keys())\n",
    "for k in ecg.keys():\n",
    "    print(k)\n",
    "    for i in ecg[k].keys():\n",
    "        print(i)\n",
    "    \n",
    "resp = nk.eda_process(eda=chest_data_dict['EDA'][baseline].flatten(), sampling_rate=700)\n",
    "resp = nk.rsp_process(chest_data_dict['Resp'][baseline].flatten(), sampling_rate=700)\n",
    "for k in resp.keys():\n",
    "    print(k)\n",
    "    for i in resp[k].keys():\n",
    "        print(i)\n",
    "    \n",
    "# For baseline, compute mean, std, for each 700 samples. (1 second values)\n",
    "file_path = os.getcwd()\n",
    "with open(file_path, \"w\") as file:\n",
    "    #file.write(str(ecg['df']))\n",
    "    file.write(str(ecg['ECG']['HRV']['RR_Intervals']))\n",
    "    file.write(\"...\")\n",
    "    file.write(str(ecg['RSP']))\n",
    "    #file.write(\"RESP................\")\n",
    "    #file.write(str(resp['RSP']))\n",
    "    #file.write(str(resp['df']))\n",
    "    #print(type(ecg['ECG']['HRV']['RR_Intervals']))\n",
    "    #file.write(str(ecg['ECG']['Cardiac_Cycles']))\n",
    "    #print(type(ecg['ECG']['Cardiac_Cycles']))\n",
    "    #file.write(ecg['ECG']['Cardiac_Cycles'].to_csv())\n",
    "    \n",
    "    \n",
    "# Plot the processed dataframe, normalizing all variables for viewing purpose\n",
    "# \"\"\"\n",
    "# \"\"\"\n",
    "bio = nk.bio_process(ecg=chest_data_dict[\"ECG\"][baseline].flatten(), rsp=chest_data_dict['Resp'][baseline].flatten(), eda=chest_data_dict[\"EDA\"][baseline].flatten(), sampling_rate=700)\n",
    "nk.z_score(bio[\"df\"]).plot()\n",
    "print(bio[\"ECG\"].keys())\n",
    "print(bio[\"EDA\"].keys())\n",
    "print(bio[\"RSP\"].keys())\n",
    "#ECG\n",
    "print(bio[\"ECG\"][\"HRV\"])\n",
    "print(bio[\"ECG\"][\"R_Peaks\"])\n",
    "#EDA\n",
    "print(bio[\"EDA\"][\"SCR_Peaks_Amplitudes\"])\n",
    "print(bio[\"EDA\"][\"SCR_Onsets\"])\n",
    "#RSP\n",
    "print(bio[\"RSP\"][\"Cycles_Onsets\"])\n",
    "print(bio[\"RSP\"][\"Cycles_Length\"])\n",
    "# \"\"\"\n",
    "print(\"Read data file\")\n",
    "#Flow: Read data for all subjects -> Extract features (Preprocessing) -> Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_path = \"../../../Data/WESAD\"\n",
    "subject = 'S4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_data = {}\n",
    "obj_data[subject] = read_data_of_one_subject(data_set_path, subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chest_data_dict = obj_data[subject].get_chest_data()\n",
    "chest_dict_length = {key: len(value) for key, value in chest_data_dict.items()}\n",
    "print(chest_dict_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels\n",
    "labels = obj_data[subject].get_labels()\n",
    "baseline = np.asarray([idx for idx,val in enumerate(labels) if val == 1])\n",
    "#print(baseline)\n",
    "\n",
    "print(\"Baseline:\", chest_data_dict['ECG'][baseline].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bio = nk.bio_process(ecg=chest_data_dict[\"ECG\"][baseline].flatten(), rsp=chest_data_dict['Resp'][baseline].flatten(), eda=chest_data_dict[\"EDA\"][baseline].flatten(), sampling_rate=700)\n",
    "nk.z_score(bio[\"df\"]).plot()\n",
    "\"\"\"print(bio[\"ECG\"].keys())\n",
    "print(bio[\"EDA\"].keys())\n",
    "print(bio[\"RSP\"].keys())\n",
    "\n",
    "#ECG\n",
    "print(bio[\"ECG\"][\"HRV\"])\n",
    "print(bio[\"ECG\"][\"R_Peaks\"])\n",
    "\n",
    "#EDA\n",
    "print(bio[\"EDA\"][\"SCR_Peaks_Amplitudes\"])\n",
    "print(bio[\"EDA\"][\"SCR_Onsets\"])\n",
    "\n",
    "#RSP\n",
    "print(bio[\"RSP\"][\"Cycles_Onsets\"])\n",
    "print(bio[\"RSP\"][\"Cycles_Length\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to display the dataframe with qgrid:\n",
    "\n",
    "Check the [quantopian link](https://github.com/quantopian/qgrid).\n",
    "\n",
    "```python\n",
    "import qgrid\n",
    "qgrid_widget = qgrid_widget.show_grid(df, show_toolbar=True)\n",
    "qgrid_widget\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive statistics in Time Series Modelling\n",
    "https://towardsdatascience.com/descriptive-statistics-in-time-series-modelling-db6ec569c0b8\n",
    "\n",
    "Stationarity\n",
    "A time series is said to be stationary if it doesn’t increase or decrease with time linearly or exponentially(no trends), and if it doesn’t show any kind of repeating patterns(no seasonality). Mathematically, this is described as having constant mean and constant variance over time. Along, with variance, the autocovariance should also not be a function of time. If you have forgotten what mean and variance are: mean is the average of the data and variance is the average squared distance from the mean.\n",
    "\n",
    "Sometimes, it’s even difficult to interpret the rolling mean visually so we take the help of statistical tests to identify this, one such being Augmented Dickey Fuller Test. ADCF Test is implemented using statsmodels in python which performs a classic null hypothesis test and returns a p-value.\n",
    "Interpretation of null hypothesis test: If p-value is less than 0.05 (p-value: low), we reject the null hypothesis and assume that the data is stationary. But if the p-value is more than 0.05 (p-value: high), then we fail to reject the null hypothesis and determine the data to be non-stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Random Forest Classifier (from jaganjag Github)\n",
    "\n",
    "*Not sure if it would be relevant but keeping the code for completeness of the repo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_data import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = execute()\n",
    "    print(data.shape)\n",
    "    X = data[:, :16]  # 16 features\n",
    "    y = data[:, 16]\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    print(y)\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(X, y,\n",
    "                                                                                test_size=0.25)\n",
    "    print('Training Features Shape:', train_features.shape)\n",
    "    print('Training Labels Shape:', train_labels.shape)\n",
    "    print('Testing Features Shape:', test_features.shape)\n",
    "    print('Testing Labels Shape:', test_labels.shape)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=5, oob_score=True)\n",
    "    clf.fit(X, y)\n",
    "    print(clf.feature_importances_)\n",
    "    # print(clf.oob_decision_function_)\n",
    "    print(clf.oob_score_)\n",
    "    predictions = clf.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    print(\"M A E: \", np.mean(errors))\n",
    "    print(np.count_nonzero(errors), len(test_labels))\n",
    "    print(\"Accuracy:\", np.count_nonzero(errors)/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_classification(n_samples=10000, n_features=6,\n",
    "                            n_informative=3, n_redundant=0,\n",
    "                            random_state=0, shuffle=True)\n",
    "\n",
    "print(X.shape)  # 10000x6\n",
    "print(y.shape)  # 10000\n",
    "\n",
    "# TODO: Feature extraction using sliding window\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(X, y,\n",
    "                                                                            test_size=0.25, random_state=42)\n",
    "# TODO: K-fold cross validation\n",
    "\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=3, oob_score=True\n",
    "                             )\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.feature_importances_)\n",
    "#print(clf.oob_decision_function_)\n",
    "print(clf.oob_score_)\n",
    "\n",
    "predictions = clf.predict(test_features)\n",
    "errors = abs(predictions - test_labels)\n",
    "print(\"M A E: \", round(np.mean(errors), 2))\n",
    "\n",
    "\n",
    "# Visualization\n",
    "feature_list = [1, 2, 3, 4, 5, 6]\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "# Pull out one tree from the forest\n",
    "tree = clf.estimators_[5]\n",
    "# Export the image to a dot file\n",
    "export_graphviz(tree, out_file='tree.dot', feature_names=feature_list, rounded=True, precision=1)\n",
    "# Use dot file to create a graph\n",
    "(graph, ) = pydot.graph_from_dot_file('tree.dot')\n",
    "# Write graph to a png file\n",
    "#graph.write_png('tree_.png')\n",
    "\n",
    "# TODO: Confusion matrix, Accuracy\n",
    "\n",
    "\n",
    "# GMM\n",
    "\n",
    "gmm = GaussianMixture(n_components=3, covariance_type='full')\n",
    "gmm.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to downsample the dataset, run a GridSearch, sort the best model according to the mean average percentage error\n",
    "\n",
    "* Downsampling of dataset: Pick 10 days in a device-specific dataset and will run the GridSearch. Allow to run the algorithm on all device-specific dataframes.\n",
    "* GridSearch trying Prophet with different training periods (8, 10 or 12 training days). This was the most critical parameter affecting the mean average percentage error (mape).\n",
    "* Sort the Prophet model according to the mape. Save the best model with graph and a dataframe containing the prediction and actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10 # Limit to 10 predictions per device.\n",
    "pred_duration = 12 # 12-day prediction\n",
    "\n",
    "for dev_nb in range(1,52):\n",
    "    device_nb = str('{:02d}'.format(dev_nb))\n",
    "    # Load the device-specific dataframe.\n",
    "    assert isinstance(device_nb, str) and len(device_nb)==2 and sum(d.isdigit() for d in device_nb)==2, 'WARNING: device_nb must be a string of 2-digits!'\n",
    "    assert int(device_nb)>=1 and int(device_nb)<=51, 'This device does not belong to the dataframe'\n",
    "    device, df_dev = load_ds(device_nb)\n",
    "    # Convert the variable device from a np.array to a string\n",
    "    regex = re.compile('[^A-Za-z0-9]')\n",
    "    device = regex.sub('', str(device))\n",
    "    \n",
    "    # Create a dataframe with the dates to use\n",
    "    dates = pd.DataFrame(columns={'date_minus_12',\n",
    "                                  'date_minus_10',\n",
    "                                  'date_minus_8',\n",
    "                                  'date_predict'})\n",
    "    dates = dates[['date_minus_12', 'date_minus_10', 'date_minus_8', 'date_predict']]\n",
    "    # List of unique dates in the dataframe\n",
    "    dates['date_minus_12'] = df_dev['ds'].unique().strftime('%Y-%m-%d')\n",
    "    dates = dates.drop_duplicates(subset=['date_minus_12'])\n",
    "    dates = dates.reset_index(drop=True)\n",
    "    # Fill the other columns and drop the 12 last columns\n",
    "    dates['date_minus_10'] = dates.iloc[2:, 0].reset_index(drop=True)\n",
    "    dates['date_minus_8'] = dates.iloc[4:, 0].reset_index(drop=True)\n",
    "    dates['date_predict'] = dates.iloc[12:, 0].reset_index(drop=True)\n",
    "    dates = dates[:-pred_duration] # Drop the 12 last rows\n",
    "    \n",
    "    # Keep only the dates with at least 12 training days\n",
    "    dates['Do_It'] = 'Do not'\n",
    "    dates['dm_12_c'] = np.nan\n",
    "    \n",
    "    for r in range(dates.shape[0]):\n",
    "        # Calculate the date_predict - pred_duration\n",
    "        date_predict = dates.iloc[r, 3]\n",
    "        date_predict = datetime.strptime(date_predict, \"%Y-%m-%d\")\n",
    "\n",
    "        date_minus_12_check = date_predict + timedelta(days=-pred_duration)\n",
    "        date_minus_12_check = datetime.strftime(date_minus_12_check, \"%Y-%m-%d\")\n",
    "        \n",
    "        # Tag the date_predict that have at least 12 training days\n",
    "        if date_minus_12_check in dates.date_predict.values or r<=11:\n",
    "            dates.iloc[r, 4] = 'yes'\n",
    "\n",
    "    dates = dates[dates.Do_It == 'yes']\n",
    "    dates.drop(['Do_It', 'dm_12_c'], axis=1)\n",
    "    \n",
    "    # Downsampling\n",
    "    if dates.shape[0]>n_samples:\n",
    "        dates = dates.sample(n=n_samples, replace=False)\n",
    "\n",
    "    # GridSearch over the (down-sampled) dataset:\n",
    "    start_time = time.time()\n",
    "    mape_table_full = pd.DataFrame()\n",
    "    \n",
    "    for r in range(dates.shape[0]):\n",
    "        # Parameters of the Grid\n",
    "        prophet_grid = {'df_dev' : [df_dev],\n",
    "                        'device' : [device],\n",
    "                        'parameter' : ['co2'],\n",
    "                        'begin' : dates.iloc[r, :3].tolist(),\n",
    "                        'end' : [dates.iloc[r, 3]],\n",
    "                        'sampling_period_min' : [1],\n",
    "                        'graph' : [1],\n",
    "                        'predict_day' : [1],\n",
    "                        'interval_width' : [0.6],\n",
    "                        'changepoint_prior_scale' : [0.01, 0.005], # list(np.arange(0.01,30,1).tolist()),\n",
    "                        'daily_fo' : [3],\n",
    "            #             'holidays_prior_scale' : list((1000,100,10,1,0.1)),\n",
    "                           }\n",
    "\n",
    "        # Run GridSearch_Prophet\n",
    "        mape_table = GridSearch_Prophet(list(ParameterGrid(prophet_grid)), metric='mape')\n",
    "        mape_table_full = mape_table_full.append(mape_table)\n",
    "\n",
    "        end_time = time.time()\n",
    "        dur_min = int((end_time - start_time)/60)\n",
    "        \n",
    "        print('Time elapsed: '+ str(dur_min) + \" minutes.\")\n",
    "\n",
    "        # Save the best model\n",
    "        print('Saving the best model')\n",
    "        \n",
    "        best_model = {'df_dev' : [df_dev],\n",
    "                      'device' : [mape_table.iloc[0, 0]],\n",
    "                      'parameter' : [mape_table.iloc[0, 1]],\n",
    "                      'begin' : [mape_table.iloc[0, 2]],\n",
    "                      'end' : [mape_table.iloc[0, 3]],\n",
    "                      'sampling_period_min' : [mape_table.iloc[0, 4]],\n",
    "                      'graph' : [1],\n",
    "                      'predict_day' : [1],\n",
    "                      'interval_width' : [mape_table.iloc[0, 5]],\n",
    "                      'changepoint_prior_scale' : [mape_table.iloc[0, 7]], # list(np.arange(0.01,30,1).tolist()),\n",
    "                      'daily_fo' : [mape_table.iloc[0, 6]],\n",
    "#                       'holidays_prior_scale' : list((1000,100,10,1,0.1)),\n",
    "                           }\n",
    "\n",
    "        # Run GridSearch_Prophet on the best model\n",
    "        mape_table = GridSearch_Prophet(list(ParameterGrid(best_model)), metric='mape')\n",
    "\n",
    "        end_time = time.time()\n",
    "        dur_min = int((end_time - start_time)/60)\n",
    "        print('Full analysis completed in '+ str(dur_min) + ' minutes.')\n",
    "\n",
    "    # Save the full table of mape_table\n",
    "    # Store the complete mape_table if this is the last prediction\n",
    "    folder_name = '/Users/guillaume/Documents/DS2020/Caru/caru/data/processed/'\n",
    "    mape_table_name = folder_name + re.sub(\"[']\", '', str(mape_table.iloc[0, 0])) + '_mape_table_full.csv'\n",
    "    mape_table_full.to_csv(mape_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# GridSearch over dataset:\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Parameters\n",
    "prophet_grid = {'df_dev' : [df_dev],\n",
    "                'device' : [device],\n",
    "                'parameter' : ['co2'],\n",
    "                'begin' : ['2019-03-20', '2019-03-22'],\n",
    "                'end' : ['2019-04-01'],\n",
    "                'sampling_period_min' : [1],\n",
    "                'graph' : [1],\n",
    "                'predict_day' : [1],\n",
    "                'interval_width' : [0.6],\n",
    "                'changepoint_prior_scale' : [0.01], # list(np.arange(0.01,30,1).tolist()),\n",
    "                'daily_fo' : [3, 6, 9],\n",
    "#                 'holidays_prior_scale' : list((1000,100,10,1,0.1)),\n",
    "               }\n",
    "\n",
    "# Run GridSearch_Prophet\n",
    "mape_table = GridSearch_Prophet(list(ParameterGrid(prophet_grid)), metric='mape')\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "dur_min = int((end_time - start_time)/60)\n",
    "\n",
    "print('GridSearch finished '+ str(dur_min) + \" minutes.\")\n",
    "print('Saving the best model')\n",
    "\n",
    "best_model = {'df_dev' : [df_dev],\n",
    "                'device' : [mape_table.iloc[0, 0]],\n",
    "                'parameter' : [mape_table.iloc[0, 1]],\n",
    "                'begin' : [mape_table.iloc[0, 2]],\n",
    "                'end' : [mape_table.iloc[0, 3]],\n",
    "                'sampling_period_min' : [mape_table.iloc[0, 4]],\n",
    "                'graph' : [1],\n",
    "                'predict_day' : [1],\n",
    "                'interval_width' : [mape_table.iloc[0, 5]],\n",
    "                'changepoint_prior_scale' : [mape_table.iloc[0, 7]], # list(np.arange(0.01,30,1).tolist()),\n",
    "                'daily_fo' : [mape_table.iloc[0, 6]],\n",
    "#                 'holidays_prior_scale' : list((1000,100,10,1,0.1)),\n",
    "               }\n",
    "\n",
    "# Run GridSearch_Prophet on the p\n",
    "mape_table = GridSearch_Prophet(list(ParameterGrid(best_model)), metric='mape')\n",
    "\n",
    "end_time = time.time()\n",
    "dur_min = int((end_time - start_time)/60)\n",
    "print('Best model saved in '+ str(dur_min) + ' minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortcuts:\n",
    "    - Move cell down: .\n",
    "    - Move cell up: /"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "prophet_anomaly_detection",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Time Series Analysis",
   "language": "python",
   "name": "time_series_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "266.198px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
